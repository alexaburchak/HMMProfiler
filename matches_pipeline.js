import { spawn } from "node:child_process";
import fs from "node:fs";
import os from "node:os";
import path from "node:path";
import readline from "node:readline";
import csv from "csv-parser";
import {
	get_config_by_path,
	get_config_path_by_args,
} from "./src/match_config.js";

/**
 * Define function to check column names
 * @param {{name: string, model_paths: string[], counts_path: string}[]} libraries
 * @returns {Promise<boolean>} - Resolves to true if all models are found, otherwise false
 */
async function checkColNames(libraries) {
	for (const library of libraries) {
		// Open counts file and read the first line
		const fileStream = fs.createReadStream(library.counts_path);
		const rl = readline.createInterface({
			input: fileStream,
			crlfDelay: Number.POSITIVE_INFINITY,
		});

		let headerLine = null;
		for await (const line of rl) {
			headerLine = line;
			break; // Only read the first line (header)
		}

		// Handle empty file case
		if (!headerLine) {
			console.error(`Error: CSV file '${library.counts_path}' is empty.`);
			return false;
		}

		// Extract column names from the CSV header
		const columns = new Set(headerLine.split(",").map((col) => col.trim()));

		// Exclude constant columns
		const constantColumns = new Set(["Count", "Total_Count", "Frequency"]);
		const modelColumns = new Set(
			[...columns].filter((col) => !constantColumns.has(col)),
		);

		// Extract expected model column names from model_paths
		const expectedColumns = new Set(
			library.model_paths.map((modelPath) =>
				path.basename(modelPath, path.extname(modelPath)),
			),
		);

		// Ensure both sets match exactly
		if (
			![...modelColumns].every((col) => expectedColumns.has(col)) ||
			![...expectedColumns].every((col) => modelColumns.has(col))
		) {
			console.error(
				`Mismatch found in '${library.counts_path}'. Expected: ${[...expectedColumns]}. Found: ${[...modelColumns]}`,
			);
			return false;
		}
	}
	return true;
}

/**
 * Define function to run hmmsearch
 * @param {string} modelPath
 * @param {string} fastaPath
 * @param {string} domtblPath
 * @param {string} stdoutPath
 * @returns {Promise<void>}
 */
async function runHMMSearch(modelPath, fastaPath, domtblPath, stdoutPath) {
	return new Promise((resolve, reject) => {
		const hmmsearch = spawn("hmmsearch", [
			"--domT",
			"150", // Domain score filter
			"--domtblout",
			domtblPath,
			modelPath,
			fastaPath,
		]);

		// Create stream for the stdout file
		const stdoutStream = fs.createWriteStream(stdoutPath, { flags: "a" });

		// Capture and write any data from stdout to the file
		hmmsearch.stdout.on("data", (data) => {
			stdoutStream.write(data);
		});

		// Capture and log any error messages generated by hmmsearch
		hmmsearch.stderr.on("data", (data) => {
			console.error(`stderr: ${data}`);
		});

		hmmsearch.on("close", (code) => {
			// Check exit code to determine if hmmsearch was successful
			if (code === 0) {
				stdoutStream.end(); // Close the stdout file stream
				resolve();
			} else {
				// If exit code is non-zero, an error occurred
				reject(new Error(`hmmsearch process exited with code ${code}`));
			}
		});
	});
}

/**
 * Define function to parse full hmmsearch output, determine best hit per target and generate a BED file
 * @param {string} domtblPath - Path to the hmmsearch domtblout file
 * @param {number} coverage - Minimum required HMM coverage
 * @param {string} bedFilePath - Path to output the BED file
 * @returns {Promise<void>}
 */
async function extractBestHMMHits(domtblPath, coverage, bedFilePath) {
	try {
		// Create readable stream for hmmer output
		const fileStream = fs.createReadStream(domtblPath, "utf8");

		// Create readline interface to read the file line by line
		const rl = readline.createInterface({
			input: fileStream,
			crlfDelay: Number.POSITIVE_INFINITY,
		});

		/** @type {Map<string, { target_name: string, score: number, ali_from: number, ali_to: number }>} */
		const bestEntries = new Map(); // Initialize map to store highest-scoring hmmsearch hit for each target sequence

		// Parse valid entries
		for await (const line of rl) {
			if (line.startsWith("#") || line.trim() === "") {
				continue; // Skip comment/empty lines
			}

			const columns = line.trim().split(/\s+/);
			if (columns.length < 23) {
				continue; // Skip lines that don't have enough columns
			}

			// Extract relevant columns
			const entry = {
				target_name: columns[0], // Target sequence name
				qlen: Number.parseInt(columns[5], 10), // Model length
				score: Number.parseFloat(columns[13]), // Bit score for each domain
				hmm_from: Number.parseInt(columns[15], 10), // HMM start
				hmm_to: Number.parseInt(columns[16], 10), // HMM end
				ali_from: Number.parseInt(columns[17], 10), // Alignment start
				ali_to: Number.parseInt(columns[18], 10), // Alignment end
			};

			// Skip entries that do not cover at least X% of the hmm
			const hmm_covered = entry.hmm_to - entry.hmm_from + 1;
			if (hmm_covered / entry.qlen < coverage) {
				continue;
			}

			// Group entries by target_name and store the highest-scoring hit
			const existing = bestEntries.get(entry.target_name);
			if (!existing || entry.score > existing.score) {
				bestEntries.set(entry.target_name, entry);
			}
		}

		// Generate the BED file content
		/** @type {string[]} bedContent - An array of strings, where each string represents a line in the BED file */
		const bedContent = [];
		for (const entry of bestEntries.values()) {
			// Format the BED file line
			const bedLine = [
				entry.target_name, // Target name
				entry.ali_from - 1, // Start position (BED format is 0-based)
				entry.ali_to, // End position
				entry.score, // Bit score
			].join("\t");
			bedContent.push(bedLine);
		}

		// Write the BED content to a file
		fs.writeFileSync(bedFilePath, bedContent.join("\n"));
	} catch (error) {
		if (error instanceof Error) {
			console.error(error.message);
		} else {
			console.error("Unknown error occurred.");
		}
		throw error;
	}
}

/**
 * Define function to trim sequences based on coordinates from HMMER
 * @param {string} inFastaFilePath - Path to input fasta file (raw protein sequences)
 * @param {string} bedFilePath - Path to BED file with trimming coordinates from each HMMER hit
 * @param {string} newHeader
 * @returns {Promise<Map<string, string>>}
 */
async function trimSeqs(inFastaFilePath, bedFilePath, newHeader) {
	return new Promise((resolve, reject) => {
		const command = `seqkit subseq --bed ${bedFilePath} ${inFastaFilePath}`;
		const seqkitSubseq = spawn(command, { shell: true });

		let output = "";
		let errorOutput = "";

		// Capture data from stdout stream (trimmed seqs)
		seqkitSubseq.stdout.on("data", (data) => {
			output += data.toString();
		});

		seqkitSubseq.stderr.on("data", (data) => {
			errorOutput += data.toString();
		});

		seqkitSubseq.on("close", (code) => {
			if (code === 0) {
				// Append the results to the output FASTA file with the model name in the header
				const cleanedOutput = output
					.split("\n")
					.map((line) => {
						if (line.startsWith(">")) {
							// Clean the header: remove everything after the first space
							return `>${newHeader}`;
						}
						return line;
					})
					.join("\n");

				// Create a Map to store the trimmed sequences with newHeader as the key
				const trimmedSequencesMap = new Map();
				let currentSequence = "";

				// Process each line in the cleaned output
				const lines = cleanedOutput.split("\n");
				for (const line of lines) {
					if (line.startsWith(">")) {
						// If a new header line is found, store the current sequence in the map
						if (currentSequence) {
							trimmedSequencesMap.set(newHeader, currentSequence);
						}
						currentSequence = ""; // Reset the current sequence
					} else {
						// Append sequence data to the current sequence
						currentSequence += line;
					}
				}

				// Store the last sequence in the map
				if (currentSequence) {
					trimmedSequencesMap.set(newHeader, currentSequence);
				}

				resolve(trimmedSequencesMap); // Return the map of trimmed sequences
			} else {
				console.error(
					`seqkit subseq failed with exit code ${code}: ${errorOutput}`,
				);
				reject(
					new Error(`Command failed with exit code ${code}: ${errorOutput}`),
				);
			}
		});

		seqkitSubseq.on("error", (err) => {
			console.error("Failed to start seqkit process:", err.message);
		});
	});
}

/**
 * Define function to process query sequences (hmmer, score filtering, trimming) into a map
 * @param {Array<{name: string, sequences: string[]}>} queryEntries - Array of query entries.
 * @param {Array<{name: string, model_paths: string[], counts_path: string}>} libraries - Array of libraries.
 * @param {number} coverage - Filter for model coverage.
 * @returns {Promise<Map<string, string>>} A promise that resolves to a Map where keys are a combination of query name and model name and values are the corresponding trimmed sequences.
 */
async function createQueryMap(queryEntries, libraries, coverage) {
	// Define temporary directories
	const mainTempDir = fs.mkdtempSync(path.join(os.tmpdir(), "main-"));
	const infastaTempDir = path.join(mainTempDir, "query_fastas");
	const domtblTempDir = path.join(mainTempDir, "hmmer_domtblout");
	const stdoutTempDir = path.join(mainTempDir, "hmmer_stdout");
	const bedFileTempDir = path.join(mainTempDir, "hmmer_bedout");

	// Create directories
	fs.mkdirSync(infastaTempDir, { recursive: true });
	fs.mkdirSync(domtblTempDir, { recursive: true });
	fs.mkdirSync(stdoutTempDir, { recursive: true });
	fs.mkdirSync(bedFileTempDir, { recursive: true });

	// Initialize a Map to store all trimmed sequences
	const allTrimmedSequences = new Map();

	// Write query sequences to fasta files for processing
	for (const entry of queryEntries) {
		const { name, sequences } = entry;
		const fastaContent = `>${name}\n${sequences.join("X".repeat(20))}\n`;
		const fastaPath = path.join(infastaTempDir, `${name}.fasta`);
		fs.writeFileSync(fastaPath, fastaContent);

		// Loop through each model in the libraries
		for (const library of libraries) {
			for (const modelPath of library.model_paths) {
				const newHeader = `${name}|${path.basename(modelPath, path.extname(modelPath))}`;
				const domtblPath = path.join(domtblTempDir, `${name}.domtbl`);
				const stdoutPath = path.join(stdoutTempDir, `${name}.stdout`);
				const bedFilePath = path.join(bedFileTempDir, `${name}.bed`);

				// Process raw query sequence with HMM search and trimming
				await runHMMSearch(modelPath, fastaPath, domtblPath, stdoutPath);
				await extractBestHMMHits(domtblPath, coverage, bedFilePath);

				// Trim sequences and store results in the Map
				const trimmedSequencesMap = await trimSeqs(
					fastaPath,
					bedFilePath,
					newHeader,
				);
				for (const [key, sequence] of trimmedSequencesMap) {
					allTrimmedSequences.set(key, sequence);
				}
			}
		}
	}

	// Return the final map of all trimmed sequences
	return allTrimmedSequences;
}

/**
 * Function to compute Levenshtein distance between two sequences
 * @param {string} a first protein sequence
 * @param {string} b second protein sequence
 * @returns {number} levenshtein distance between sequence a and sequence b
 */
function levenshteinDistance(a, b) {
	// Create 2D array
	const dp = Array(a.length + 1)
		.fill(null)
		.map(() => Array(b.length + 1).fill(null));

	// Initialize the first column
	for (let i = 0; i <= a.length; i++) {
		dp[i][0] = i;
	}

	// Initialize the first row
	for (let j = 0; j <= b.length; j++) {
		dp[0][j] = j;
	}

	for (let i = 1; i <= a.length; i++) {
		for (let j = 1; j <= b.length; j++) {
			// Determine if amino acids are the same (cost 0) or different (cost 1)
			const cost = a[i - 1] === b[j - 1] ? 0 : 1;

			// if different, take the minimum
			dp[i][j] = Math.min(
				dp[i - 1][j] + 1, // deletion
				dp[i][j - 1] + 1, // insertion
				dp[i - 1][j - 1] + cost, // substitution
			);
		}
	}
	return dp[a.length][b.length];
}

/**
 * Reads a CSV file and searches for the closest matching protein sequences
 * @param {string} csvFile - CSV file path
 * @param {string} libName - Library name
 * @param {Map<string, Map<string, string>>} queries - Map of query names to models and corresponding trimmed query sequences
 * @param {number} [maxLevenshteinDistance=Infinity] - Optional filter on Levenshtein distance
 * @param {Map<string, Array<Object>>} finalMap - Map with trimmed query sequences as keys and its corresponding matches as the value
 * @returns {Promise<void>}
 */
async function findClosestMatches(
	csvFile,
	libName,
	queries,
	maxLevenshteinDistance = Number.POSITIVE_INFINITY,
	finalMap = new Map(), // Initialize an empty map if not provided
) {
	/** @type {Array<Record<string, string>>} */
	const records = []; // Array to store all rows from the CSV

	return new Promise((resolve, reject) => {
		fs.createReadStream(csvFile)
			.pipe(csv())
			.on("data", (row) => {
				// Store each row from the CSV as an object
				if (row && typeof row === "object") {
					records.push(row);
				}
			})
			.on("end", () => {
				// Iterate over each query (queryName -> modelName -> querySeq)
				for (const [queryName, queryMap] of queries.entries()) {
					// Compute Levenshtein distances for each CSV row
					const matches = records
						.map((row) => {
							/** @type {Record<string, any>} tempResult - Base object for one match result */
							const tempResult = { Query: queryName };
							let allWithinThresh = true; // Flag to ensure all distances are under max_LD

							// Loop through all modelName -> querySeq pairs in the queryMap
							for (const [modelName, querySeq] of queryMap) {
								const sequence = row[modelName];
								const dist = levenshteinDistance(querySeq, sequence);

								// Store sequence and LD for this model
								tempResult[`${modelName}_Match`] = sequence;
								tempResult[`${modelName}_LD`] = dist;

								// If any distance exceeds threshold, discard the entire match
								if (dist > maxLevenshteinDistance) {
									allWithinThresh = false;
									break;
								}
							}

							if (!allWithinThresh) { return null };

							// Append additional row info from CSV
							return {
								...tempResult,
								Library: libName,
								Count: Number(row.Count),
								Total_Count: Number(row.Total_Count),
								Frequency: Number(row.Frequency),
							};
						})
						.filter(Boolean); // Remove any null results

					// Accumulate matches for each queryName
					const existingMatches = finalMap.get(queryName) || [];
					finalMap.set(queryName, /** @type {Object[]} */([...existingMatches, ...matches]));
				}
				resolve();
			})
			.on("error", reject);
	});
}

/**
 * Function to write data to a CSV file
 * @param {Map<string, Array<Record<string, any>>>} data
 * @param {string} outputPath
 */
function writeCSV(data, outputPath) {
	if (data.size === 0) {
		console.log("No data to write.");
		return;
	}

	// Collect all unique headers across all objects in the map
	const allHeaders = new Set();
	for (const [, rows] of data) {
		for (const row of rows) {
			for (const key of Object.keys(row)) {
				allHeaders.add(key);
			}
		}
	}

	const headers = Array.from(allHeaders);

	// Convert map data to CSV format
	const csvRows = [
		headers.join(","), // Header row
	];

	// Flatten the rows and create CSV data
	for (const [, rows] of data) {
		for (const row of rows) {
			csvRows.push(
				headers
					.map((col) => (row[col] === undefined ? "NA" : row[col]))
					.join(","),
			);
		}
	}

	// Write CSV to file
	fs.writeFileSync(outputPath, csvRows.join("\n"), "utf8");
}

// Main logic
async function main() {
	// Read config
	const config_path = await get_config_path_by_args();
	const config = get_config_by_path(config_path);
	console.log(config);
	if (config === null) {
		return; // exit program if config is null
	}

	// Extract parameters from config object
	const { max_LD, hmm_coverage, queryEntries, libraries, output_path } = config;

	// Check that all models are present as columns in counts_path
	const modelsValid = await checkColNames(libraries);
	if (!modelsValid) {
		// Terminate if models do not match counts_path columns
		console.error(
			`Model names and library columns do not match. Exiting pipeline.`,
		);
		process.exit(1);
	}

	// Process query sequences into a map
	const allTrimmedSequences = await createQueryMap(queryEntries, libraries, hmm_coverage);

	// Group sequences by queryName
	const groupedQueries = new Map();

	for (const [queryKey, sequence] of allTrimmedSequences.entries()) {
		const [queryName, modelName] = queryKey.split("|");
		if (!groupedQueries.has(queryName)) {
			groupedQueries.set(queryName, new Map());
		}
		groupedQueries.get(queryName).set(modelName, sequence);
	}

	// Final map to store all the matches
	const finalMap = new Map();

	for (const library of libraries) {
		await findClosestMatches(
			library.counts_path,
			library.name,
			groupedQueries,
			max_LD,
			finalMap,
		);
	}

	console.log(finalMap);

	// Write map to CSV
	writeCSV(finalMap, output_path);
	console.log("CSV of sequence matches saved to:", output_path);
}

// Execute main function
main().catch(console.error);
